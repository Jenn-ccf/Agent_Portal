# Agent_Portal

## å°ˆæ¡ˆæ¶æ§‹åœ–
```
agent_portal/
â”‚ 
â”œâ”€â”€ config.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ log_utils.py
â”‚   â”œâ”€â”€ ocr_utils.py
â”‚   â”œâ”€â”€ embedding_utils.py
â”‚   â””â”€â”€ get_file_utils.py
â”‚ 
â”‚   # ocr -> chunking -> embedding -> qdrant
â”œâ”€â”€ indexer/
â”‚   â”œâ”€â”€ chunking.py
â”‚   â””â”€â”€ indexer_pipeline.py
â”‚
â”‚   # summary -> embedding -> qdrant
â”œâ”€â”€ summary/
â”‚   â”œâ”€â”€ summary.py
â”‚   â””â”€â”€ summary_pipeline.py
â”‚
â”‚   # vector search
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ search.py
â”‚   â”œâ”€â”€ rerank.py
â”‚   â”œâ”€â”€ retrieve_pipeline.py
â”‚   â””â”€â”€ api.py
â”‚ 
â”‚   # agent flow
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ search_tools.py
â”‚   â”œâ”€â”€ mrkl.py  
â”‚   â”œâ”€â”€ agent_pipeline.py 
â”‚   â””â”€â”€ api.py
â”‚ 
â”‚   # streamlit UIä»‹é¢
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ kw_mapping.json
â”‚   â”œâ”€â”€ intent.py
â”‚   â””â”€â”€ app.py 
â”‚
â”‚   # qdrant db
â”œâ”€â”€ db/
â”‚   â””â”€â”€ collection/
â”‚       â”œâ”€â”€ chunk_product-overview
â”‚       â”œâ”€â”€ summary_product-overview
â”‚       â”œâ”€â”€ chunk_other
â”‚       â””â”€â”€ summary_other
â”‚
â”‚   # embedding model: BGE-M3
â”œâ”€â”€ model/
â”‚
â”œâ”€â”€ pdf_files/
â”‚   â”œâ”€â”€ product-overview/
â”‚   â”‚   â”œâ”€â”€ <pdf_1>
â”‚   â”‚   â”œâ”€â”€ <pdf_2>
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ json_files/
â”‚   â”‚   â”‚   â”œâ”€â”€ <json_1>
â”‚   â”‚   â”‚   â””â”€â”€ <json_2>
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ chunk_json_files/
â”‚   â”‚   â”‚   â”œâ”€â”€ <chunk_json_1>
â”‚   â”‚   â”‚   â””â”€â”€ <chunk_json_2>
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ summary_json_files/
â”‚   â”‚   â”‚   â”œâ”€â”€ <summary_json_1>
â”‚   â”‚   â”‚   â””â”€â”€ <summary_json_2>
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ logs/
â”‚   â”‚       â”œâ”€â”€ ocr_logs/      
â”‚   â”‚       â”‚   â””â”€â”€ indexer_ocr.log 
â”‚   â”‚       â”œâ”€â”€ chunk_logs/
â”‚   â”‚       â”‚   â””â”€â”€ indexer_chunk.log  
â”‚   â”‚       â”œâ”€â”€ summary_logs/
â”‚   â”‚       â”‚   â””â”€â”€ summary.log 
â”‚   â”‚       â””â”€â”€ embed_logs/
â”‚   â”‚           â”œâ”€â”€ indexer_embed.log   
â”‚   â”‚           â””â”€â”€ summary_embed.log  
â”‚   â”‚ 
â”‚   â”œâ”€â”€ application-and-medical/    
â”‚   â”œâ”€â”€ customer-policy-service/  
â”‚   â””â”€â”€ other/ 
â”‚ 
â”‚   # YAMLs for EKS
â””â”€â”€ eks/
    â”œâ”€â”€ qdrant-service.yaml
    â”œâ”€â”€ indexer-job.yaml
    â”œâ”€â”€ summary-job.yaml
    â”œâ”€â”€ retriever-deployment.yaml
    â”œâ”€â”€ agent-deployment.yaml
    â”œâ”€â”€ frontend-deployment.yaml
    â”œâ”€â”€ frontend-service.yaml
    â”œâ”€â”€ s3-pvc.yaml
    â”œâ”€â”€ s3-sc.yaml
    â”œâ”€â”€ ebs-pvc.yaml
    â”œâ”€â”€ ebs-sc.yaml
    â”œâ”€â”€ trust-policy.json
    â””â”€â”€ iam-policy.json
```


## utils

```
â”œâ”€â”€ config.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ log_utils.py
â”‚   â”œâ”€â”€ ocr_utils.py
â”‚   â”œâ”€â”€ embedding_utils.py
â”‚   â””â”€â”€ get_file_utils.py
```

---

### `log_utils.py`

> è™•ç† log ç›¸é—œçš„å‡½å¼

- `create_time_log_entry()`
- `log_to_file()`
- `get_processed_files()`

---

#### `create_time_log_entry(filename: str, description: str, start_time: float) -> str`

- ç›®çš„ï¼šè¼¸å…¥æª”æ¡ˆåç¨±ã€æè¿°ã€è™•ç†é–‹å§‹æ™‚é–“ï¼Œä»¥å»ºç«‹æ—¥èªŒæ¢ç›®

#### `log_to_file(log_entry: str, log_file_path: str) -> None`

- ç›®çš„ï¼šå°‡æ—¥èªŒæ¢ç›®æ·»åŠ åˆ°æŒ‡å®šçš„æ—¥èªŒæª”æ¡ˆä¸­
- ä¸»è¦é‚è¼¯ï¼š
    - è®€å– log æª”ä¸¦åˆªé™¤
    - å°‡å°±å…§å®¹èˆ‡æ–°å¢å…§å®¹åˆä½µ
    - é–‹å•Ÿæ–°æª”æ¡ˆå°‡å…§å®¹å¯«å…¥

#### `get_processed_files(log_file_path: str) -> Set[str]`

- ç›®çš„ï¼šè®€å– log æª”æ¡ˆï¼Œç²å–å·²è™•ç†éçš„æª”æ¡ˆåˆ—è¡¨

---

### `ocr_utils.py`

> å°‡ PDF æª”æ¡ˆç¶“é OCR å¾Œå­˜æˆ JSON æª”

- `merge_tables_by_position()`
- `extract_tables_and_text()`
- `convert_pdf_to_json()`

```mermaid
graph TD
    subgraph ocr_utils.py 
        A["<b>convert_pdf_to_json</b><br/>(ä¸»è¦é€²å…¥é»)<br/>å°‡PDFè½‰ç‚ºçµæ§‹åŒ–JSON"] --> B["<b>extract_tables_and_text</b><br/>å¾PDFæå–æ–‡å­—èˆ‡è¡¨æ ¼"];
        B --> C["<b>merge_tables_by_position</b><br/>åˆä½µä½ç½®ç›¸è¿‘çš„è¡¨æ ¼"];
        B -- "ä½¿ç”¨" --> D[/"PyMuPDF (fitz)<br/>è®€å–PDFæ–‡å­—å€å¡Š"/];
        B -- "ä½¿ç”¨" --> E[/"img2table<br/>å¾åœ–ç‰‡ä¸­æå–è¡¨æ ¼"/];
        E -- "ä¾è³´" --> F[/"TesseractOCR<br/>é€²è¡Œå…‰å­¸å­—å…ƒè¾¨è­˜"/];
    end
```

---

#### `merge_tables_by_position(tables_dict:  Dict[int, List[ExtractedTable]]) -> Dict[int, List[ExtractedTable]]`

- ç›®çš„ï¼šå°‡åŒä¸€é é¢ä¸­ã€Œä½ç½®ç›¸é„°ã€æ‡‰è¦–ç‚ºåŒä¸€å¼µè¡¨æ ¼ã€çš„è¡¨æ ¼ç‰©ä»¶åˆä½µ
- ä¸»è¦é‚è¼¯ï¼š
    - é€é è™•ç†è¡¨æ ¼æ¸…å–®
    - é€ä¸€æª¢æŸ¥ç›¸é„°è¡¨æ ¼æ˜¯å¦æ‡‰åˆä½µ
        - ç¸±å‘è·é›¢å°ï¼ˆ<100 pxï¼‰
        - æ¬„ä½æ•¸å·®ç•°å°ï¼ˆ<=1ï¼‰
        - å·¦é‚Šç•Œ x1 ç›¸è¿‘ï¼ˆ<100 pxï¼‰
    - è¡¨æ ¼åˆä½µ
        - åˆä½µå…©å€‹è¡¨æ ¼çš„ contentï¼ˆä¾åŸé †åºé‡æ–°ç´¢å¼•ï¼‰
        - BBox é‡æ–°åŒ…ä½ä¸Šä¸‹å…©æ®µï¼Œæˆç‚ºä¸€å€‹å®Œæ•´è¡¨æ ¼
        - Title åˆä½µï¼šå…©è€…éƒ½æœ‰å°±ä¸²èµ·ä¾†ï¼›å…¶ä¸­ä¸€è€…æœ‰å°±æ²¿ç”¨
    - è·³éå·²åˆä½µçš„è¡¨æ ¼ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹çµ„
    - è¼¸å‡ºè¡¨æ ¼

#### `extract_tables_and_text(pdf_path: str) -> Tuple[Dict[int, List[str]], Dict[int, List[pd.DataFrame]]]`

- ç›®çš„ï¼šOCR ä¸»è¦è™•ç†æµç¨‹ï¼šå¾ PDF ä¸­æå–è¡¨æ ¼å’Œæ–‡å­—
- ä¸»è¦é‚è¼¯ï¼š
    - åˆå§‹åŒ– OCR èˆ‡ PDF
    - æå–è¡¨æ ¼ä¸¦è™•ç†ï¼ˆèª¿ç”¨ `merge_tables_by_position()`ï¼‰
    - é€é è™•ç† PDF
    - å®šä½æ¯å€‹è¡¨æ ¼çš„ BBox
    - æ“·å–é é¢æ–‡å­—ä¸¦æ’é™¤è¡¨æ ¼å€å¡Š
    - è¼¸å‡ºæ•´ç†å¾Œçš„å…§å®¹
        - ç´”æ–‡å­— texts_per_pageï¼ˆä¸å«è¡¨æ ¼å…§æ–‡å­—ï¼‰
        - è¡¨æ ¼ DataFrame tables_per_page

#### `convert_pdf_to_json(pdf_path: str, json_path: str, with_tables: bool = True) -> bool`

- ç›®çš„ï¼šå°‡ PDF ç”¨ OCR è™•ç†å­˜ JSON æª”
- ä¸»è¦é‚è¼¯ï¼š
    - PDF æ–‡å­—èˆ‡è¡¨æ ¼æå–ï¼ˆèª¿ç”¨ `extract_tables_and_text()`ï¼‰
    - é€é æ•´ç†å…§å®¹
    - ç”Ÿæˆ JSON çµæ§‹
        ```
        [
            {
                "filename":"åœ˜éšªå…¨é«”å—ç›Šäººè²æ˜æ›¸æš¨å—ç›Šç³»çµ±è¡¨-1120101.pdf",
                "page": 1,
                "content":"è²æ˜æ›¸ ä¸€ã€ç«‹æ›¸äººå…¨é«”å› å—å±±äººå£½ä¿éšªè‚¡ä»½..."
            },
            {
                "filename":"åœ˜éšªå…¨é«”å—ç›Šäººè²æ˜æ›¸æš¨å—ç›Šç³»çµ±è¡¨-1120101.pdf",
                "page": 2,
                "content":"å£½éšªæ¥­å±¥è¡Œå€‹äººè³‡æ–™ä¿è­·æ³•å‘ŠçŸ¥ç¾©å‹™å…§å®¹..."
            },
            ...
        ]
        ```

---

### `embedding_utils.py`

> é€²è¡Œå‘é‡åŒ–å’Œä¸Šå‚³è‡³è³‡æ–™åº«ï¼Œåˆ†ç‚º chunk, summary å…©ç¨®

åµŒå…¥è™•ç†é¡åˆ¥ `class EmbeddingProcessor`
- `__init__()`
- `_generate_point_id()`
- `_embed_text()`
- `_upsert_points()`
- `chunk_embedding_upsert()`
- `summary_embedding_upsert()`

```mermaid
graph TD
    subgraph embedding_utils.py
        direction TB
        A["<b>chunk_embedding_upsert</b><br/>(ä¸»è¦é€²å…¥é»)<br/>æ‰¹æ¬¡è™•ç†æ–‡æª”åˆ‡å¡Š"]
        B["<b>summary_embedding_upsert</b><br/>(ä¸»è¦é€²å…¥é»)<br/>è™•ç†å–®ä¸€æ‘˜è¦"]

        subgraph "å…§éƒ¨è¼”åŠ©å‡½å¼"
            direction LR
            C["<b>_embed_text</b><br/>å°‡æ–‡å­—å‘é‡åŒ–"]
            D["<b>_generate_point_id</b><br/>ç”¢ç”Ÿå”¯ä¸€çš„è³‡æ–™é»ID"]
            E["<b>_upsert_points</b><br/>ä¸Šå‚³è³‡æ–™é»åˆ°è³‡æ–™åº«"]
        end

        A --> C;
        A --> D;
        A --> E;

        B --> C;
        B --> D;
        B --> E;
    end

    subgraph log_utils.py
        F[/"create_time_log_entry<br/> log_to_file"/];
    end

    C -- "è¨˜éŒ„æ—¥èªŒ" --> F;
    E -- "è¨˜éŒ„æ—¥èªŒ" --> F;
```


---

#### `__init__(self, embedding_model, client)`

- ç›®çš„ï¼šè¼¸å…¥å¯¦ä¾‹åŒ–æ¨¡å‹è·Ÿå‘é‡è³‡æ–™åº«å®¢æˆ¶ç«¯

#### `_generate_point_id(self, filename: str, page: int, index: int, search_type: str) -> str`

- ç›®çš„ï¼šé€éè¼¸å…¥çš„åƒæ•¸è³‡è¨Šç”Ÿæˆå”¯ä¸€çš„é» ID

#### `_embed_text(self, texts: List[str], filename: str, desc: str, log_file_path: str) -> List[List[float]]`

- ç›®çš„ï¼šå°æ–‡æœ¬åˆ—è¡¨é€²è¡Œå‘é‡åŒ–ï¼Œä¸¦è¨˜éŒ„æ—¥èªŒ

#### `_upsert_points(self, collection: str, points: List[PointStruct], filename: str, desc: str, log_file_path: str) -> None`

- ç›®çš„ï¼šå°‡é»åˆ—è¡¨ä¸Šå‚³åˆ° Qdrantï¼Œä¸¦è¨˜éŒ„æ—¥èªŒ

#### `chunk_embedding_upsert(self, chunks: List[Dict], filename: str, collection: str, log_file_path: str, batch_size: int=config.chunk_batch_size) -> bool`

- ç›®çš„ï¼šå°åˆ‡å¡Šæ–‡æœ¬é€²è¡Œå‘é‡åŒ–ä¸¦ä¸Šå‚³åˆ° Qdrant
- ä¸»è¦é‚è¼¯ï¼š
    - è®€å–è¼¸å…¥çš„ chunks: List[Dict]
    - åˆ†æˆå°æ‰¹æ¬¡
    - é€²è¡Œå‘é‡åŒ–
    - å»ºç«‹è³‡æ–™é»
    - ä¸Šå‚³è³‡æ–™åº«

#### `summary_embedding_upsert(self, summary_data: Dict, collection: str, log_file_path: str) -> bool`

- ç›®çš„ï¼š è®€å–å–®ä¸€æ‘˜è¦ JSONï¼Œå‘é‡åŒ–å¾Œä¸Šå‚³åˆ° Qdrant
- ä¸»è¦é‚è¼¯ï¼š
    - è®€å–è¼¸å…¥çš„ summary_data: Dict
    - åˆ†æˆå°æ‰¹æ¬¡
    - é€²è¡Œå‘é‡åŒ–
    - å»ºç«‹è³‡æ–™é»
    - ä¸Šå‚³è³‡æ–™åº«

---

### `get_file_utils.py`

> - å–å¾—è³‡æ–™å¤¾è·¯å¾‘
> - å¤šè³‡æ–™å¤¾è™•ç†å‡½å¼

- `get_folder_paths()`
- `get_pdf_folders()`
- `process_all_folders()`

---

#### `get_folder_paths(folder_name: str) -> dict`

- ç›®çš„ï¼šæ ¹æ“šè³‡æ–™å¤¾åç¨±ï¼Œè¿”å›ç›¸é—œçš„è·¯å¾‘å­—å…¸
- ä¸»è¦é‚è¼¯ï¼š
    - PDF å­è³‡æ–™å¤¾è·¯å¾‘
    - ocr å¾Œçš„ json æª”æ¡ˆè·¯å¾‘
    - chunking å¾Œçš„ json æª”æ¡ˆè·¯å¾‘
    - summary å¾Œçš„ json æª”æ¡ˆè·¯å¾‘
    - indexer ocr log æª”æ¡ˆè·¯å¾‘
    - indexer chunking logæª”æ¡ˆè·¯å¾‘
    - summary log æª”æ¡ˆè·¯å¾‘
    - indexer embedding log æª”æ¡ˆè·¯å¾‘
    - summary embedding log æª”æ¡ˆè·¯å¾‘
    - å­è³‡æ–™å¤¾å°æ‡‰çš„ chunk collection name
    - å­è³‡æ–™å¤¾å°æ‡‰çš„ summary collection name

#### `get_pdf_folders(folder_path: str) -> list`

- ç›®çš„ï¼šç²å– PDF åŸºç¤ç›®éŒ„ä¸‹çš„æ‰€æœ‰è³‡æ–™å¤¾

#### `process_all_folders(processing_method: callable) -> None`

- ç›®çš„ï¼šä½¿ç”¨æŒ‡å®šçš„è™•ç†æ–¹æ³•è™•ç† PDF åŸºç¤ç›®éŒ„ä¸‹çš„æ‰€æœ‰è³‡æ–™å¤¾ï¼ˆèª¿ç”¨`get_pdf_folders()`ï¼‰

---

## indexer

```
â”œâ”€â”€ indexer/
â”‚   â”œâ”€â”€ chunking.py
â”‚   â””â”€â”€ indexer_pipeline.py
```
```mermaid
graph TD
    A["å–å¾—æ‰€æœ‰ PDF è³‡æ–™å¤¾"] --> B["éæ­·è³‡æ–™å¤¾
    å–å¾—æ‰€æœ‰ PDF æª”æ¡ˆ"];
    B --> C["OCR
    (çµæœå­˜ç‚º JSON æª”)"];
    subgraph "å–®ä¸€ PDF æª”æ¡ˆè™•ç†æµç¨‹"
        C --> D["Chunking
        (çµæœå­˜ç‚º chunk JSON æª”)"];
        D --> E["Embedding"];
        E --> F["Upsert to Qdrant"]
    end
```

---

### `chunking.py`

> å°‡ OCR å¾Œçš„åŸå§‹ JSON æª”æ¡ˆå…§å®¹é€²è¡Œåˆ‡å¡Šï¼Œå†å­˜ç‚º chunk JSON æª”


- `create_chunks()`
- `process_json_to_chunks()`

```mermaid
graph TD
    subgraph "chunking.py (tool)"
        A["<b>process_json_to_chunks</b><br/>(ä¸»è¦é€²å…¥é»)<br/>å­˜ç‚º chunk JSON"] --> B["<b>create_chunks</b>"];    
    end
```

---

#### `create_chunks(text: str, chunk_size: int = 1000) -> List[str]`

- ç›®çš„ï¼šå°‡æ–‡æœ¬åˆ†å¡Š
- ä¸»è¦é‚è¼¯ï¼š
    - ä¾æ®µè½åˆæ­¥åˆ†å¡Šï¼š
        - å°‡æ–‡æœ¬ä»¥ \n åˆ†æ®µ
        - å°‡æ®µè½ç´¯ç©åˆ° current_chunkï¼Œç›´åˆ°é•·åº¦æ¥è¿‘ chunk_size â†’ å½¢æˆåˆæ­¥ chunk
    - è‹¥åˆæ­¥ chunk é•·åº¦ä»è¶…é chunk_sizeï¼š
        - ä¾å¥è™Ÿã€Œã€‚ã€åˆ‡åˆ†æˆå°å¥
        - é€å¥ç´¯ç©æˆæ–° chunkï¼Œç¢ºä¿æ¯å€‹ chunk ä¸è¶…é chunk_size

#### `process_json_to_chunks(input_json_path: str, output_json_path: str, metadata: List[str] = None) -> bool`

- ç›®çš„ï¼šå°‡ JSON æª”å…§å®¹åˆ‡ chunkï¼Œå¦å­˜æ–°çš„ chunk JSON æª”
- ä¸»è¦é‚è¼¯ï¼š
    - è®€å–è¼¸å…¥çš„ JSON æª”æ¡ˆ
    - é€é è™•ç†å…§å®¹
    - åˆ‡åˆ†æˆ chunkï¼ˆèª¿ç”¨ `create_chunks()`ï¼‰
    - ç”Ÿæˆ JSON çµæ§‹
        ```
        {
            "chunk_1": {
                "filename": "é•·ç…§ä¿éšªå•†å“æ¢æ¬¾.txt",
                "page": 1,
                "content": "å—å±±äººå£½å¢å¿ƒé™ªä¼´é•·æœŸç…§é¡§ä¿éšª(ä¸€æ¬¡çµ¦ä»˜)_PILTC å—å±±äººå£½ä¿éšªè‚¡ä»½æœ‰é™å…¬å¸..."
            },
            "chunk_2": {
                "filename": "é•·ç…§ä¿éšªå•†å“æ¢æ¬¾.txt",
                "page": 2,
                "content": "å—å±±äººå£½å¢å¿ƒé™ªä¼´é•·æœŸç…§é¡§ä¿éšª(ä¸€æ¬¡çµ¦ä»˜)_PILTC ä¿‚æŒ‡ä¾ç¬¬åäºŒæ¢ç´„å®š..."
            },
            "chunk_3":{
            ...
            }
        }
        ```

---

### `indexer_pipeline.py`

>  è™•ç† Indexer æ•´é«”æµç¨‹

- `Indexer` æµç¨‹é¡åˆ¥
    - `__init__()`
    - `process_folder()`
- ä¸»ç¨‹å¼å…¥å£`main()` 

```mermaid
graph TD
    subgraph indexer_pipeline.py 
    subgraph "main() ä¸»ç¨‹å¼å…¥å£"
        
        I1["å¯¦ä¾‹åŒ– QdrantClient & <br>Embedding model"] --> I2["å¯¦ä¾‹åŒ– EmbeddingProcessor"];
        I2 --> I3["å¯¦ä¾‹åŒ– Indexer"];
        I3 --> B["get_file_utils.process_all_folders(indexer.process_folder)"];
    end
    subgraph "indexer.process_folder"
        direction LR
        C["ocr_utils.convert_pdf_to_json"] --> D["chunking.process_json_to_chunks"];
        D --> E["embedding_utils.EmbeddingProcessor.chunk_embedding_upsert"];
    end
    B -- "é‡è¤‡å‘¼å«" --> C;
    end
    subgraph log_utils.py
        L[/"create_time_log_entry<br/> log_to_file"/];
    end
    C --> L;
    D --> L
```
---

#### `__init__(self, embedding_processor, client)`

- ç›®çš„ï¼šè¼¸å…¥å¯¦ä¾‹åŒ–æ¨¡å‹è·Ÿå‘é‡è³‡æ–™åº«å®¢æˆ¶ç«¯

#### `process_folder(self, folder_name: str) -> Optional[Dict[str, int]]`

- ç›®çš„ï¼šè™•ç†å–®ä¸€è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ PDF æª”æ¡ˆ
- ä¸»è¦é‚è¼¯ï¼š
    - ç²å–å„æª”æ¡ˆè·¯å¾‘ï¼Œéæ¿¾å·²è™•ç†æª”æ¡ˆ
    - å–®ä¸€æª”æ¡ˆæœƒé€ä¸€é€²è¡Œï¼šOCR -> Chunking -> Embedding -> Upsert
    - çµ±è¨ˆæœ€çµ‚çµæœï¼Œå„æ­¥é©ŸæˆåŠŸï¼†å¤±æ•—æ•¸é‡çµ±è¨ˆ

#### `main()`

- ç›®çš„ï¼šé€£æ¥ Qdrantã€è¼‰å…¥æ¨¡å‹ï¼Œè™•ç† PDF åŸºç¤ç›®éŒ„åº•ä¸‹æ‰€æœ‰è³‡æ–™å¤¾ï¼ˆèª¿ç”¨`process_all_folders()`ï¼‰

---

## summary

```
â”œâ”€â”€ summary/
â”‚   â”œâ”€â”€ summary.py
â”‚   â””â”€â”€ summary_pipeline.py
```

```mermaid
graph TD
    A["å–å¾—æ‰€æœ‰ OCR å®Œçš„ JSON è³‡æ–™å¤¾"] --> B["éæ­·è³‡æ–™å¤¾
    å–å¾—æ‰€æœ‰ JSON æª”æ¡ˆ"];
    B --> C["Summary
    (çµæœå­˜ç‚º JSON æª”)"];
    subgraph "å–®ä¸€ JSON æª”æ¡ˆè™•ç†æµç¨‹"
        C --> D["Embedding"];
        D --> E["Upsert to Qdrant"]
    end
```

---

### `summary.py`

> å°‡ OCR å¾Œçš„åŸå§‹ JSON æª”æ¡ˆå…§å®¹é€²è¡Œæ‘˜è¦ï¼Œå†å­˜ç‚º summary JSON æª”

- `summarize_text()`
- `summarize_document_from_json()`

```mermaid
graph TD
    subgraph "summary.py (tool)"
        A["<b>summarize_document_from_json</b><br/>(ä¸»è¦é€²å…¥é»)<br/>å­˜ç‚º summary JSON"] --> B["<b>summarize_text</b>"];
        
    end
```

---

#### `summarize_text(text: str) -> Optional[Dict[str, Any]]`

- ç›®çš„ï¼šå‘¼å«æ¨¡å‹ç”¢ç”Ÿæ–‡ç« æ‘˜è¦
- ä¸»è¦é‚è¼¯ï¼š
    - prompt è¨­å®š
    - å®šç¾©æ¨¡å‹å›å‚³æ ¼å¼
        ```
        {
            "title": "æ–‡ä»¶æ¨™é¡Œ",
            "file_type": "æª”æ¡ˆé¡å‹",
            "metadata": [æ–‡æª”ä¸»è¦å…ƒç´ åˆ—è¡¨],
            "intent": [ç›¸é—œæ„åœ–åˆ—è¡¨],
            "summary": "å…§å®¹æ‘˜è¦"
        }
        ```
    - æ¨¡å‹è«‹æ±‚è¨­å®š
    - å‘¼å«æ¨¡å‹

#### `summarize_document_from_json(input_json_path: str, output_json_path: str) -> Optional[Dict[str, Any]]`

- ç›®çš„ï¼šå°‡ JSON æª”å…§å®¹å–æ‘˜è¦ï¼Œå¦å­˜æ–°çš„ summary JSON æª”
- ä¸»è¦é‚è¼¯ï¼š
    - è®€å–è¼¸å…¥çš„ JSON æª”æ¡ˆ
    - åˆä½µæ¯ä¸€é çš„å…§å®¹
    - å‘¼å«æ¨¡å‹é€²è¡Œæ‘˜è¦ï¼ˆèª¿ç”¨`summarize_text()`ï¼‰
    - å­˜æˆ JSON æª”
        ```
        {
          "title": "è²æ˜æ›¸",
          "file_type": "ç”³è«‹æ›¸è¡¨å–®",
          "metadata": [
            "æ–‡å­—",
            "è¡¨æ ¼",
            "è¡¨å–®æ¬„ä½",
            "ç°½åæ¬„ä½",
            "å‹¾é¸é …ç›®"
          ],
          "intent": [
            "ç†è³ è¦ç¯„",
            "ç”³è«‹æ›¸è¡¨å–®è²æ˜æ›¸"
          ],
          "summary": "æ­¤ç‚ºå—å±±äººå£½ä¿éšªè‚¡ä»½æœ‰é™å…¬å¸åœ˜é«”éšªè¢«ä¿éšªäººèº«æ•…å¾Œï¼Œå—ç›Šäººç”³è«‹ç†è³ æ‰€éœ€å¡«å¯«çš„è²æ˜æ›¸ã€‚...",
          "filename": "åœ˜éšªå…¨é«”å—ç›Šäººè²æ˜æ›¸æš¨å—ç›Šç³»çµ±è¡¨-1120101.pdf"
        }
        ```

---

### `summary_pipeline.py`

>  è™•ç† Summary æ•´é«”æµç¨‹

- `Summary` æµç¨‹é¡åˆ¥
    - `__init__()`
    - `process_folder()`
- ä¸»ç¨‹å¼å…¥å£`main()` 

```mermaid
graph TD
    subgraph summary_pipeline.py 
    subgraph "main() ä¸»ç¨‹å¼å…¥å£"
        
        I1["å¯¦ä¾‹åŒ– QdrantClient & <br>Embedding model"] --> I2["å¯¦ä¾‹åŒ– EmbeddingProcessor"];
        I2 --> I3["å¯¦ä¾‹åŒ– Indexer"];
        I3 -->  B["get_file_utils.process_all_folders(summary.process_folder)"];
    end
    subgraph "summary.process_folder"
        direction LR
        C["summary.summarize_document_from_json"] --> D["embedding_utils.EmbeddingProcessor.summary_embedding_upsert"];
    end
    B -- "é‡è¤‡å‘¼å«" --> C;
    end
    subgraph log_utils.py
        L[/"create_time_log_entry<br/> log_to_file"/];
    end
    C --> L
```

---


#### `__init__(self, embedding_processor, client)`

- ç›®çš„ï¼šè¼¸å…¥å¯¦ä¾‹åŒ–æ¨¡å‹è·Ÿå‘é‡è³‡æ–™åº«å®¢æˆ¶ç«¯

#### `process_folder(self, folder_name: str) -> Optional[Dict[str, int]]`

- ç›®çš„ï¼šè™•ç† OCR å¾Œ JSON è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ JSON æª”æ¡ˆ
- ä¸»è¦é‚è¼¯ï¼š
    - ç²å–å„æª”æ¡ˆè·¯å¾‘ï¼Œéæ¿¾å·²è™•ç†æª”æ¡ˆ
    - å–®ä¸€æª”æ¡ˆæœƒé€ä¸€é€²è¡Œï¼šSummary -> Embedding -> Upsert
    - çµ±è¨ˆæœ€çµ‚çµæœï¼Œå„æ­¥é©ŸæˆåŠŸï¼†å¤±æ•—æ•¸é‡çµ±è¨ˆ

#### `main()`

- ç›®çš„ï¼šé€£æ¥ Qdrantã€è¼‰å…¥æ¨¡å‹ï¼Œè™•ç† PDF åŸºç¤ç›®éŒ„åº•ä¸‹æ‰€æœ‰è³‡æ–™å¤¾ï¼ˆèª¿ç”¨`process_all_folders()`ï¼‰


---

## retriever

```
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ search.py
â”‚   â”œâ”€â”€ rerank.py
â”‚   â”œâ”€â”€ retrieve_pipeline.py
â”‚   â””â”€â”€ api.py
```

```mermaid
flowchart TD
    
    B1[Initialize embedding<br>model] --> B2[å¯¦ä¾‹åŒ–<br>searcher & reranker]
    B2--> D["Search (tool)"]
    subgraph "retrieve_pipeline.py"
        D --> Ee[process search results]
        Ee -- chunk --> F["filter by<br>similarity threshold"]
        Ee -- summary --> E["Rerank (tool)"]
        E --> F
    end
        F --> H[Result Output]
    
    subgraph "search.py (tool)"
        D1[Connect to Qdrant DB]
        D2[Query Vectorize]
        D3[Vector Search]
        D1 --> D2 --> D3
    end
    subgraph "rerank.py (tool)"
        E1[Title Vectorize]
        E2[Cosine Similarity]
        E1 --> E2
    end
    
    D3 -...-> D
    E2 -...-> E
    D2 -...-> E2
```
---


### `search.py`

> - å°‡ä½¿ç”¨è€…æŸ¥è©¢å‘é‡åŒ–
> - åŸ·è¡Œ Qdrant å‘é‡æœç´¢

- å‘é‡æœå°‹é¡åˆ¥ `class VectorSearch`
    - `__init__()`
    - `embed_query()`
    - `_build_category_filter()`
    - ä¸»è¦æœå°‹æ–¹æ³•ï¼š`search()`

```mermaid
graph TD
    subgraph "å‘é‡æœå°‹åŠŸèƒ½VectorSearch" 
        A(search) --> B{_build_category_filter};
        A --> D(embed_query);
        A --> E["qdrant_client.query_points"];
    end
    D -...->|query embedding| E;
    D ~~~ E
    

    style E fill:#fff,stroke:#d66,stroke-width:2px,color:#a00
```

---

#### `__init__(self, embedding_model: Any)`

- ç›®çš„ï¼šå®šç¾© Embedding æ¨¡å‹ã€å‘é‡è³‡æ–™åº«é€£ç·š


#### `embed_query(self, text: str) -> List[float]`

- ç›®çš„ï¼šå°‡æ–‡å­—è½‰æ›ç‚ºå‘é‡

#### `_build_category_filter(self, categories: List[str] = None) -> Filter`

- ç›®çš„ï¼šå»ºç«‹ metadata é¡åˆ¥éæ¿¾æ¢ä»¶
- ä¸»è¦é‚è¼¯ï¼š
    - æ¥æ”¶ categories åˆ—è¡¨ä½œç‚ºéæ¿¾ä¾æ“š
    - éæ­· config.PDF_METADATAï¼Œæ‰¾å‡ºå°æ‡‰ category çš„ PDF åç¨±
    - å¦‚æœæ‰¾åˆ°ç¬¦åˆçš„ PDF å‰‡å»ºç«‹ Qdrant Filter
#### `search(self, query: str, top_k: int, search_type: str, collection: str, categories: List[str] = None) -> Tuple[List[float], List[Any]]`

- ç›®çš„ï¼šåŸ·è¡Œå‘é‡ç›¸ä¼¼åº¦æœå°‹
- ä¸»è¦é‚è¼¯ï¼š
    - if any, å»ºç«‹éæ¿¾æ¢ä»¶
    - æŸ¥è©¢å‘é‡åŒ–
    - åŸ·è¡Œ Qdrant æœå°‹
    - è¿”å› query embedding ï¼† æœå°‹çµæœ


---

### `rerank.py`

> - å°‡ä½¿ç”¨è€…æŸ¥è©¢èˆ‡æ–‡ä»¶æ¨™é¡Œè¨ˆç®— cosine similarity é€²è¡Œé‡æ’åº

- å‘é‡æœå°‹é¡åˆ¥ `class Rerank`
    - `__init__()`
    - `_cosine_similarity()`
    - ä¸»è¦é‡æ’åºæ–¹æ³•ï¼š`rerank_by_title()`

```mermaid
graph TD
    subgraph "é‡æ’åºåŠŸèƒ½ Rerank" 
        A(rerank_by_title) --> B(_cosine_similarity)
    end
    
```
---

#### `__init__(self, embedding_model: Any)`

- ç›®çš„ï¼šå®šç¾© Embedding æ¨¡å‹

#### `_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float`

- ç›®çš„ï¼šè¨ˆç®—å…©å€‹å‘é‡çš„ cosine similarity

#### `rerank_by_title(self, query_vector: List[float], results: List[Dict[str, Any]]) -> List[Dict[str, Any]]`

- ç›®çš„ï¼šæ ¹æ“šæ–‡ä»¶æ¨™é¡Œï¼ˆtitleï¼‰èˆ‡æŸ¥è©¢ï¼ˆqueryï¼‰ç›¸é—œæ€§é€²è¡Œé‡æ’åº
- ä¸»è¦é‚è¼¯ï¼š
    - æ¥æ”¶å‰ä¸€æ­¥æœå°‹çµæœ
    - å°‡æ–‡ä»¶æ¨™é¡Œï¼ˆtitleï¼‰å‘é‡åŒ–
    - è¨ˆç®—æ–‡ä»¶æ¨™é¡Œï¼ˆtitleï¼‰èˆ‡æŸ¥è©¢ï¼ˆqueryï¼‰çš„ cosine similarity
    - é‡æ’åºå¾Œè¼¸å‡ºçµæœ

---

### `retrieve_pipeline.py`

> - æ•´åˆæª¢ç´¢æµç¨‹
> - ä¾æ“šæœå°‹é¡å‹æ±ºå®šæ˜¯å¦è¦é‡æ’åº
> - ç›¸é—œæ€§åˆ†æ•¸é–¾å€¼ç¯©é¸

- æª¢ç´¢å™¨é¡åˆ¥
    - `__init__()`
    - `_process_search_results()`
    - ä¸»è¦æª¢ç´¢æµç¨‹ï¼š`retrieve()`

```mermaid
flowchart TD
    
    B1[Initialize embedding<br>model] --> B2[å¯¦ä¾‹åŒ–<br>searcher & reranker]
    B2--> D["searcher"]
    subgraph "retrieve"
        D --> Ee(_process_search_results)
        Ee -- chunk --> F["filter by<br>similarity threshold"]
        Ee -- summary --> E["reranker"]
        E --> F
    end
    
```
---


#### `__init__(self, search_tool: Any, rerank_tool: Any) -> None`
- ç›®çš„ï¼šåˆå§‹åŒ–æª¢ç´¢å™¨ï¼Œæ³¨å…¥ä¾è³´çš„å·¥å…·å¯¦ä¾‹
    - searcher
    - reranker

#### `_process_search_results(self, search_type: str, search_result: List[Any]) -> List[Dict[str, Any]]`
- ç›®çš„ï¼šè™•ç†æœå°‹çµæœï¼Œåˆ†æˆ chunk èˆ‡ summary å…©ç¨®æ ¼å¼
- ä¸»è¦é‚è¼¯ï¼šæ¥æ”¶ search_resultï¼Œä¸¦ä¾æ“šä¸åŒæŸ¥è©¢é¡å‹çš„è³‡æ–™ payload æ•´ç†æˆå°æ‡‰çš„çµæœå½¢å¼
    - chunk
        ```
        result = {
            'similarity_score': point.score,
            'filename': point.payload.get('filename', ''),
            'page': point.payload.get('page', ''),
            'content_preview': point.payload.get('content', '')[:200] + "..." if len(point.payload.get('content', '')) > 200 else point.payload.get('content', ''),
            'full_content': point.payload.get('content', '')
        }
        ```
    - summary
        ```
        result = {
            'similarity_score': point.score,
            'filename': point.payload.get('filename', ''),
            'title': point.payload.get('title', ''),
            'file_type': point.payload.get('file_type'),
            'metadata': point.payload.get('metadata', []),
            'summary': point.payload.get('summary', '')
        }
        ```


#### `retrieve(self, query: str, threshold_score: float, top_k: int, collection: str, search_type: str, categories: List[str] = None) -> Dict[str, Union[List[Any], int]]`

- ç›®çš„ï¼šæ•´åˆæª¢ç´¢æµç¨‹
- ä¸»è¦é‚è¼¯ï¼š
    - åŸ·è¡Œå‘é‡æœå°‹ï¼ˆsearcherï¼‰
    - è™•ç†æœå°‹çµæœæ ¼å¼ï¼ˆ`_process_search_results()`ï¼‰
    - æ ¹æ“šæœå°‹é¡å‹é€²è¡Œå¾ŒçºŒè™•ç†
        - chunkï¼šç›´æ¥é€²è¡Œç›¸é—œæ€§åˆ†æ•¸é–¾å€¼éæ¿¾
        - summaryï¼šé€²è¡Œé‡æ’åºï¼ˆrerankerï¼‰åœ¨åšé–¾å€¼éæ¿¾
    - å›å‚³å½¢å¼ï¼š
        ```
        # final output format
        result_dict = {
            "results": results, 
            "total_count": len(results)
        }
        ```


---

### `api.py`

> åŸºæ–¼ FastAPI çš„ RAG æª¢ç´¢æœå‹™ï¼Œæ¥æ”¶å‰ç«¯æŸ¥è©¢è«‹æ±‚ï¼Œèª¿ç”¨æª¢ç´¢å™¨ä¸¦è¿”å›çµæœã€‚

- `lifespan()`
- `class QueryRequest(BaseModel)`
- `retrieve()`

#### `lifespan(app: FastAPI)`

- ç›®çš„ï¼šéåŒæ­¥è³‡æºç”Ÿå‘½é€±æœŸç®¡ç†ï¼Œç¢ºä¿æ ¸å¿ƒè³‡æºåœ¨æœå‹™å•Ÿå‹•æ™‚è¢«é«˜æ•ˆåœ°åˆå§‹åŒ–ï¼Œä¸¦åœ¨æœå‹™é—œé–‰æ™‚å®‰å…¨é‡‹æ”¾ã€‚
- ä¸»è¦é‚è¼¯ï¼š
    - è¼‰å…¥ Embedding Model
    - å¯¦ä¾‹åŒ– Searcher & Reranker
    - åˆå§‹åŒ– RAG æª¢ç´¢å™¨


#### `retrieve(req: QueryRequest, request: Request) -> Dict[str, Union[List[Any], int]]`

- ç›®çš„ï¼šä½¿ç”¨æª¢ç´¢å™¨è™•ç†æœå°‹è«‹æ±‚ä¸¦è¿”å›çµæœ
- ä¸»è¦é‚è¼¯ï¼š
    - è·¯ç”±å®šç¾©
    - ç²å–æª¢ç´¢å™¨å¯¦ä¾‹
    - æœå‹™ç‹€æ…‹æª¢æŸ¥
    - åŸ·è¡Œæª¢ç´¢é‚è¼¯
- ä»‹é¢è¦æ ¼ï¼š
    `POST /retrieve`
    ```
    json = {
        query: str  # æŸ¥è©¢å…§å®¹
        top_k: int = config.TOP_K  # è¿”å›çµæœæ•¸é‡
        threshold_score: float = config.THRESHOLD_SCORE  # ç›¸ä¼¼åº¦é–¾å€¼
        collection: Optional[str]   # æŸ¥è©¢ intentï¼Œç”¨ä¾†æŒ‡å®š collection åç¨±
        search_type: str  # æŸ¥è©¢é¡å‹ï¼šchunk æˆ– summary
    }
    ```
- å›å‚³ç‹€æ…‹ï¼š
    - 200 OKï¼šæª¢ç´¢æˆåŠŸ -> æˆåŠŸè¿”å›åŒ…å«æœå°‹çµæœå’Œè¨ˆæ•¸çš„å­—å…¸
    - 503 Service Unavailableï¼šæœå‹™æœªå•Ÿå‹• -> retriever å°šæœªåˆå§‹åŒ–å®Œæˆï¼Œæœå‹™ä¸å¯ç”¨ã€‚
    - 500 Internal Server Errorï¼šå…§éƒ¨éŒ¯èª¤ -> æª¢ç´¢æœå‹™åœ¨åŸ·è¡Œ retrieve éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸçš„ç•°å¸¸ã€‚
- æš´éœ² API ç«¯å£ï¼š
    ```
    uvicorn api:app --host 0.0.0.0 --port 8000
    ```
    `api:app` â†’ api æ˜¯æª”åï¼ˆä¸å« .pyï¼‰ï¼Œapp æ˜¯ FastAPI å¯¦ä¾‹åç¨±ã€‚
- ä½¿ç”¨ç¯„ä¾‹ï¼š
    ```
    url = "http://0.0.0.0:8000/retrieve"
    payload = {
        "query": "ç™Œç—‡ä¿éšª ç–¾ç—…ç­‰å¾…æœŸé–“",
        "top_k": 20,
        "threshold_score": 0.4,
        "search_type": search_type,
        "collection": "form"
    }

    response = requests.post(url, json=payload)
    ```

## agent

```
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ search_tools.py
â”‚   â”œâ”€â”€ mrkl.py  
â”‚   â”œâ”€â”€ agent_pipeline.py 
â”‚   â””â”€â”€ api.py
```

```mermaid
graph TD
    subgraph "ä¸»åŸ·è¡Œæµç¨‹ agent_pipeline.py"
        A(agent_flow) --å¯¦ä¾‹åŒ–--> B(MRKLAgent __init__);
        A --èª¿ç”¨--> C(agent.query);
    end

    subgraph "Agent æ¶æ§‹ mrkl.py"
        B --> D(_setup_tools);
        B --> E(_setup_agent);
        B --> F[ChatBedrockConverse];
        E --> K[initialize_agent];
        D --> G(Tool: Form Search);
        D --> H(Tool: Product Search);
        D --> I(Tool: Customer Policy Search);
        D --> J(Tool: Medical Search);
    end

    subgraph "å·¥å…·å‡½å¼ search_tools.py"
        
        G -- wraps --> L(search_form);
        H -- wraps --> M(search_product);
        I -- wraps --> N(search_customer_policy);
        J -- wraps --> O(search_medical);
    end

    
        C -- invokes --> P{LangChain Agent Loop};
        P -- decides --> G;
        P -- decides --> H;
        P -- decides --> I;
        P -- decides --> J;
    
    
    subgraph "å¤–éƒ¨ API å‘¼å«"
        L & M & N & O --> Q["requests.post to Retriever API"];
        K -- uses --> F;
    end

    style F fill:#cff,stroke:#333,stroke-width:1px,stroke-dasharray: 5 5
    style K fill:#cff,stroke:#333,stroke-width:1px,stroke-dasharray: 5 5
    style P fill:#ffc,stroke:#333,stroke-width:2px
    style Q fill:#ffc,stroke:#333,stroke-width:2px
```

---

### `search_tools.py`

> å»ºç«‹ agent æœƒç”¨åˆ°çš„æœç´¢å·¥å…·ï¼Œåˆ†ç‚ºå››å€‹è³‡æ–™é›†çš„å°æ‡‰å·¥å…·

- `search_form()`ï¼šæœç´¢ã€Œå…¶ä»–å„é …è¡¨å–®ã€ï¼ˆchunk_otherï¼‰
- `search_product()`ï¼šæœç´¢ã€Œå•†å“ç¸½è¦½ã€ï¼ˆchunk_product-overviewï¼‰
- `search_customer_policy()`ï¼šæœç´¢ã€Œå®¢æˆ¶æœå‹™ä¿å–®æœå‹™ã€ï¼ˆchunk_costomer-policy-serviceï¼‰
- `search_medical()`ï¼šæœç´¢ã€ŒæŠ•ä¿èˆ‡é†«å‹™ã€ï¼ˆchunk_application-and-medicalï¼‰

---

#### `search_form(query: str) -> str` å’Œå…¶ä»–ä¸‰å€‹ collection æœå°‹å‡½å¼

- ç›®çš„ï¼šæœç´¢æŒ‡å®š Qdrant collection
- ä¸»è¦é‚è¼¯ï¼š
    - å‘¼å« Retriever API
    - æ•´ç†å›å‚³çµæœ
        ```
        formatted = (
            f"ğŸ“„ æ–‡ä»¶ {i}\n"
            f"   ä¾†æº: {result.get('filename', 'æœªçŸ¥')} (ç¬¬{result.get('page', 'N/A')}é )\n"
            f"   ç›¸ä¼¼åº¦: {result.get('similarity_score', 0):.3f}\n"
            f"   å®Œæ•´å…§å®¹: {result.get('full_content', '').strip()}\n"
            "------------------------------------------------------------"
        )
        ```


---

### `mrkl.py`

> å»ºæ§‹ agent æ¨¡å‹åŠå…¶æ€è€ƒæ¡†æ¶

`class MRKLAgent` MRKLæ¡†æ¶ agent é¡åˆ¥
- `__init__()`
- `_setup_tools()`
- `_setup_agent()`
- `query()`

```mermaid
graph TD
    B(MRKLAgent __init__)
    B --> D("_setup_tools()");
    B --> E("_setup_agent()");
    B --> F
    B --> Query
    subgraph "è¨­ç½®å·¥å…·"
        
        D --> G(Tool: Form Search);
        D --> H(Tool: Product Search);
        D --> I(Tool: Customer Policy Search);
        D --> J(Tool: Medical Search);
    end
    
    subgraph "è¨­ç½®æ¨¡å‹"
        
        F[ChatBedrockConverse];
        E --> K[initialize_agent];
    end
    
    subgraph "æŸ¥è©¢åŠŸèƒ½"
        Query("query()<br>invoke agent è™•ç†æŸ¥è©¢")
    end
    

    subgraph "å·¥å…·å‡½å¼ search_tools.py"
        G -- wraps --> L(_search_form);
        H -- wraps --> M(_search_product);
        I -- wraps --> N(_search_customer_policy);
        J -- wraps --> O(_search_medical);
    end

    
    
    subgraph "å¤–éƒ¨ API å‘¼å«"
        L & M & N & O --> Q["requests.post to Retriever API"];
        K -- uses --> F;
    end

    style F fill:#cff,stroke:#333,stroke-width:1px,stroke-dasharray: 5 5
    style K fill:#cff,stroke:#333,stroke-width:1px,stroke-dasharray: 5 5
    style Q fill:#ffc,stroke:#333,stroke-width:2px
```

---

#### `__init__(self)`

- ç›®çš„ï¼šåˆå§‹åŒ– MRKL Agentï¼Œä½¿ç”¨ Amazon Bedrock LLM

#### `_setup_tools(self) -> None`

- ç›®çš„ï¼šå°‡å››å€‹æœå°‹çš„å‡½å¼è¨­å®šç‚º Agent çš„å·¥å…·

#### `_setup_agent(self) -> None`

- ç›®çš„ï¼šè¨­ç½® Agent
- ä¸»è¦é‚è¼¯ï¼š`initialize_agent()`
    - è¨­å®šå·¥å…·
    - è¨­å®š LLM
    - è¨­å®š Agent type
    - è¨­å®š Agent åƒæ•¸
    - è¨­å®šæœ€å¤§è¿­ä»£æ¬¡æ•¸
    - è¨­å®šæœ€å¤§åŸ·è¡Œæ™‚é–“
    - è¨­å®šèˆ‡ã€Œä¸­é–“è¼¸å‡ºæ­¥ã€ç›¸é—œçš„åƒæ•¸

#### `query(self, question: str) -> Tuple[List[Dict[str, str]], str]`

- ç›®çš„ï¼šè™•ç†æŸ¥è©¢ï¼Œè¿”å›ä¸­é–“æ­¥é©Ÿå’Œæœ€çµ‚ç­”æ¡ˆ
- ä¸»è¦é‚è¼¯ï¼š
    - å‘¼å« Agent åŸ·è¡Œæ€è€ƒ
        - Think: å•é¡Œé€²ä¾†è¦èª¿ç”¨å“ªå€‹å·¥å…·ï¼ˆæŸ¥å“ªå€‹collectionï¼‰
        - Action: using tools
            - `search_product_tool`ï¼šæœå°‹ `chunk_product-overview` collection
            - `search_policy_tool`ï¼šæœå°‹ `chunk_customer-policy-service` collection
            - `search_medical_tool`ï¼šæœå°‹ `chunk_application-and-medical` collection
            - `search_form_tool`ï¼šæœå°‹ `chunk_other` collection
        - Observation: è§€å¯Ÿæœå°‹çµæœï¼Œæ±ºå®šé‚„éœ€è¦å†ä½¿ç”¨ä»€éº¼å·¥å…· 
    - è™•ç†ä¸­é–“æ­¥é©Ÿç‚ºå­—å…¸ `step`ï¼Œå­˜ç‚ºåˆ—è¡¨ `inter_steps`
        ```
        step = {
            "thought": agent_action.log.split("\n")[0],
            "action": agent_action.tool,
            "action_input": agent_action.tool_input,
            "observation": str(observation)
        }
        ```
        ```
        inter_steps = [
            {
                "thought": "æˆ‘éœ€è¦ä½¿ç”¨ search_form_tool ä¾†æŸ¥è©¢å¤©æ°£ã€‚",
                "action": "search_form_tool",
                "action_input": {"location": "å°åŒ—"},
                "observation": "æ°£è±¡å±€è³‡æ–™é¡¯ç¤ºä»Šå¤©å¤šé›²ï¼Œæº«åº¦ä»‹æ–¼ 22 åˆ° 28 åº¦ã€‚"
            },
            {
                "thought": "æˆ‘å·²å–å¾—è³‡æ–™ï¼Œç¾åœ¨å¯ä»¥å›ç­”ä½¿ç”¨è€…ã€‚",
                "action": "Final Answer",
                "action_input": None,
                "observation": None
            },
            {
                "final_answer": "ä»Šå¤©å°åŒ—å¤šé›²ï¼Œæ°£æº«ç´„ 22ï½28 åº¦ã€‚"
            }
        ]
        ```
---

### `agent_pipeline.py`

> æ•´åˆ agent æœå°‹ã€å›ç­”æµç¨‹

- `agent_flow()`

---

#### `agent_flow(query: str) -> Dict[str, Any]`

- ç›®çš„ï¼šåŸ·è¡Œ MRKL Agentï¼Œå„²å­˜æ•´å€‹æŸ¥è©¢éç¨‹å’Œæœ€çµ‚çµæœ
- ä¸»è¦é‚è¼¯ï¼š
    - å°‡æ‰€æœ‰éç¨‹å„²å­˜ç‚ºåˆ—è¡¨ `steps`
    - å¯¦ä¾‹åŒ– Agent ï¼Œèª¿ç”¨æŸ¥è©¢åŠŸèƒ½ï¼Œå°‡è¿”å›çš„ä¸­é–“æ­¥å­˜å…¥ `steps`
        ```
        steps = [
            {"start": "é–‹å§‹ Agent åŸ·è¡Œ..."},
            {
                "thought": "...",
                "action": "...",
                "action_input": "...",
                "observation": "..."
            },
            {
                "thought": "...",
                "action": "...",
                "action_input": "...",
                "observation": "..."
            },
            ...
            {"final_answer": "..."},
            {"end": "Agent åŸ·è¡Œå®Œæˆ"}
        ]
        ```
    - è¿”å›æœ€å¾Œçµæœ
        ```
        result_dict = {
            "query": query,
            "steps": steps,
            "final_result": final_answer
        }
        ```

---


### `api.py`

> åŸºæ–¼ FastAPI çš„ RAG æª¢ç´¢æœå‹™ï¼Œæ¥æ”¶å‰ç«¯æŸ¥è©¢è«‹æ±‚ï¼Œèª¿ç”¨ agent ä¸¦è¿”å›çµæœã€‚

- `class QueryRequest(BaseModel)`
- `ask_agent()`

#### `ask_agent(req: QueryRequest) -> Dict[str, Any]`

- ç›®çš„ï¼šå‘¼å« agent è™•ç†æœå°‹è«‹æ±‚ä¸¦è¿”å›çµæœ
- ä¸»è¦é‚è¼¯ï¼š
    - è·¯ç”±å®šç¾©
    - å‘¼å« agent é€²è¡ŒæŸ¥è©¢ï¼ˆèª¿ç”¨`agent_pipeline.agent_flow`ï¼‰
- ä»‹é¢è¦æ ¼ï¼š
    `POST /agent`
    ```
    json = {
        query: str  # æŸ¥è©¢å…§å®¹
    }
    ```
- æš´éœ² API ç«¯å£ï¼š
    ```
    uvicorn api:app --host 0.0.0.0 --port 8001
    ```
- ä½¿ç”¨ç¯„ä¾‹ï¼š
    ```
    url = "http://0.0.0.0:8001/agent"
    payload = {
        "query": "ç™Œç—‡ä¿éšª ç–¾ç—…ç­‰å¾…æœŸé–“"
    }

    response = requests.post(url, json=payload)
    ```

---

## frontend

```
â”‚   # streamlit UIä»‹é¢
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ kw_mapping.json
â”‚   â”œâ”€â”€ intent.py
â”‚   â””â”€â”€ app.py 
```

```mermaid
flowchart TD

subgraph Frontend["Streamlit App"]
    A[app.py<br>Streamlit UI]
    B[intent.py<br>Intent Classifier]
    C[kw_mapping.json<br>Keyword Mapping]
end

A <--->|Call & Return intent| B
C -.->|Load keywords| B

%% Output from classifier goes to backend
A -->|Choose backend| D[Call Retriever or Agent]
```

---

### `kw_mapping.json`

- æ„åœ–ç·¨è™Ÿèˆ‡å…¶å¯¦éš›æ„åœ–
    - intent 1: æŸ¥è©¢å•†å“èˆ‡è¡ŒéŠ·è³‡è¨Š
    - intent 2: æŸ¥è©¢æŠ•ä¿/æ ¸ä¿/é†«å‹™ç›¸é—œæ–‡ä»¶
    - intent 3: æŸ¥è©¢ç†è³ æœå‹™ç›¸é—œè¦ç¯„
    - intent 4: æŸ¥è©¢å¥‘ç´„èˆ‡ä¿å–®è®Šæ›´
    - intent 5: æŸ¥è©¢ç¹³è²»èˆ‡æ”¶è²»ç®¡ç†ç›¸é—œæ–‡ä»¶
    - intent 6: æŸ¥è©¢å¢å“¡/çµ„ç¹”ç™¼å±•/è¼”å°ç›¸é—œæ–‡ä»¶
    - intent 7: ä¸‹è¼‰ç”³è«‹æ›¸/è¡¨å–®/è²æ˜æ›¸
    - intent 8: æŸ¥è©¢åˆ¶åº¦/è¦ç¯„/çå‹µè¾¦æ³•
    - intent 9: æŸ¥è©¢EåŒ–ç›¸é—œæ–‡ä»¶æˆ–æ“ä½œæ‰‹å†Š
    - intent 10: æŸ¥è©¢å…¬å¸è³‡è¨Š/æ–°è/åˆŠç‰©
- æ¯å€‹é—œéµå­—å¯èƒ½æœƒå°æ‡‰åˆ°ä¸€å€‹ä»¥ä¸Šçš„æ„åœ–ï¼Œå¾Œé¢å¤¾å¸¶ä¸åŒçš„ç›¸é—œè©
    ```
    {
        "keyword":"é«˜é½¡",
        "intent":1,
        "related_words":"é«˜é½¡ä¿éšœã€é«˜é½¡ç¤¾æœƒå°ˆæ¡ˆã€é«˜é½¡é†«ç™‚æ„å¤–ã€éæŠ•è³‡å‹ç›¸é—œ"
    },
    {
        "keyword":"é«˜é½¡",
        "intent":7,
        "related_words":"é«˜é½¡æŠ•ä¿è¦ç¯„ï¼Œé«˜é½¡æŠ•ä¿è©•ä¼°é‡è¡¨ã€é«˜é½¡è¦ä¿æ›¸(æ¨‚é½¡)"
    },
    ```

---

### `intent.py`



> - è®€å– kw_mapping.json
> - å°è¼¸å…¥ query é€²è¡Œæ–·è©
> - æ ¹æ“šé—œéµå­—æŸ¥æ‰¾å°æ‡‰çš„æ„åœ–å’Œç›¸é—œè©


#### jieba æ–·è©

- ä¸»è¦é‚è¼¯ï¼šæŠ½å–é—œéµå­—èˆ‡æœå°‹æ„åœ–é…å°ç¸½è¡¨ `kw_mapping.json` è£¡çš„é—œéµå­—é€²è¡Œå»é‡ï¼Œå»ºç«‹ jieba æ–·è©çš„è‡ªå®šç¾©å­—å…¸
    

#### `expand_query(query: str) -> Tuple[bool, list]`
- ç›®çš„ï¼šåŒ¹é… query ä¸­æ˜¯å¦æœ‰å°æ‡‰åˆ°é—œéµå­—æ„åœ–
- ä¸»è¦é‚è¼¯ï¼š
    - å…ˆå° query é€²è¡Œ jieba æ–·è©
    - åŒ¹é…æ–·è©çµæœçš„åˆ—è¡¨ä¸­æ˜¯å¦æœ‰åŒ¹é…åˆ°é—œéµå­—
    - ä½¿ç”¨ `config.INTENT_COLLECTION_MAP` é…å°æœå°‹æ„åœ–èˆ‡ç›¸é—œè©
        ```
        INTENT_COLLECTION_MAP = {
            "1": "product-overview",
            "2": "application-and-medical",
            "3": "customer-policy-service",
            "4": "customer-policy-service",
            "5": "customer-policy-service",
            "6": "customer-policy-service",
            "7": "other",
            "8": "other",
            "9": "other",
            "10": "other"   
        }
        ```
        ```
        results = [
            (intent_collection, expanded_query),
            (intent_collection, expanded_query),
            ...
        ]
        # example
        results = [
            ('product-overview', 'ä¿éšªé‡‘ ä¿é¡ ä¿éšªé‡‘é¡ åŸºæœ¬ä¿é¡'),
            ('application-and-medical', 'ä¿éšªé‡‘ ç†è³ é‡‘ ä¿éšªçµ¦ä»˜ çµ¦ä»˜é‡‘ ä¿éšªé‡‘æ‰£æŠµé†«ç™‚è²» ä¿éšªé‡‘æ‰£æŠµé†«ç™‚è²»æˆæ¬Šæ›¸ ä¿éšªé‡‘æ‰£æŠµé†«ç™‚è²»æœå‹™æµç¨‹ ä¿éšªé‡‘æ‰£æŠµé†«ç™‚è²»ç›¸é—œå•é¡Œ')
        ]        
        ```


---

### `app.py`


> - å»ºç«‹ä½¿ç”¨è€…ä»‹é¢è®“ä½¿ç”¨è€…èƒ½å¤ è¼¸å…¥æŸ¥è©¢å•é¡Œ
> - åˆ†ç‚º Agent æˆ– Document Search é€²è¡ŒæŸ¥è©¢ï¼Œæœ€å¾Œé¡¯ç¤ºçµæœä¸¦æ”¶é›†ä½¿ç”¨è€…å›é¥‹

- æŸ¥è©¢åŠŸèƒ½
    - `run_agent()`
    - `run_document_search()`
- çµæœå±•ç¤º
    - `display_agent_results()`
    - `display_document_results()`
- å›é¥‹è™•ç†
    - `save_feedback()`
- App é é¢è¨­å®š
    - Tab 1ï¼šæ™ºèƒ½å•ç­”
    - Tab 2ï¼šè©•åƒ¹çµ±è¨ˆ


```mermaid
graph TD
    subgraph "ä¸»æŸ¥è©¢æµç¨‹"
        A[User Clicks 'Search' Button] --> B{Query Length > 8?};
        B -- Yes --> C(run_agent);
        B -- No --> D(intent.expand_query);
        D --> E{Intent Found?};
        E -- Yes<br>Search matched collections--> F(run_document_search);
        E -- No<br>Search all collections --> F;
        C --> H(display_agent_results);
        F --> I(display_document_results);
    end

    subgraph "è©•åƒ¹ç´€éŒ„"
        I --> L[User Clicks Feedback Button];
        L --> M(save_feedback);
    end

```




---

#### `run_agent(query: str) -> Tuple[list, str]`

- ç›®çš„ï¼šå‘¼å« Agent APIï¼Œè¿”å› steps èˆ‡æœ€çµ‚ç­”æ¡ˆ


#### `run_document_search(intent_classify: tuple) -> Tuple[List[Dict], int]`

- ç›®çš„ï¼šå‘¼å« Retriever APIï¼Œè¿”å› final_results èˆ‡ total_counts
- ä¸»è¦é‚è¼¯ï¼š
    - intent_classify è£¡å¯èƒ½æœ‰å¤šå€‹`(intent, expand query)`ï¼Œæœƒä¾æ“šæ„åœ–è¿­ä»£ï¼Œåˆ°æŒ‡å®šçš„ collection é€²è¡Œæœå°‹
    - å°‡æ‰€æœ‰æª¢ç´¢å›ä¾†çš„æ–‡ä»¶éƒ½å­˜åœ¨ final_results åˆ—è¡¨

#### `display_agent_results(agent_steps: list) -> None`

- ç›®çš„ï¼šæ¥æ”¶ Agent API å›å‚³çµæœï¼Œæ•´ç†æˆåœ¨ Streamlit å‘ˆç¾çš„å½¢å¼
- ä¸»è¦é‚è¼¯ï¼š
    - Thouhgt
    - Action
    - Action Input
    - Observation
        - retrieved documents
    - Final Answer 

#### `display_document_results(query: str, results: dict, total_count: int, similarity_threshold: int) -> None`

- ç›®çš„ï¼šæ¥æ”¶ Retriever API å›å‚³çµæœï¼Œæ•´ç†æˆåœ¨ Streamlit å‘ˆç¾çš„å½¢å¼
- ä¸»è¦é‚è¼¯ï¼š
    - å°æ‰€æœ‰æ–‡ä»¶çš„ç›¸é—œæ€§åˆ†æ•¸åšæ’åº
    - é é¢å‘ˆç¾
        - file name
        - similarity score
        - title
        - summary
    - æä¾›å›é¥‹æŒ‰éˆ•ï¼šæœ‰å¹«åŠ©/æ²’å¹«åŠ©

#### `save_feedback(query: str, pdf_name: str, rating: str) -> None`

- ç›®çš„ï¼šå„²å­˜ä½¿ç”¨è€…å›é¥‹åˆ° JSON æª”æ¡ˆ
- ä¸»è¦é‚è¼¯ï¼š
    - åœ¨ä½¿ç”¨è€…æŒ‰ä¸‹å›é¥‹æŒ‰éˆ•æ™‚è§¸ç™¼å›é¥‹å„²å­˜æ©Ÿåˆ¶
    - å„²å­˜è³‡æ–™ç‚ºï¼šæ™‚é–“æˆ³è¨˜ã€ä½¿ç”¨è€…æŸ¥è©¢å•é¡Œã€ä½¿ç”¨è€…å›é¥‹ä¹‹æ–‡ä»¶ã€ä½¿ç”¨è€…è©•åƒ¹ï¼ˆæœ‰å¹«åŠ©/æ²’å¹«åŠ©ï¼‰
        ```
        feedback_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "query": query,
            "pdf_name": pdf_name,
            "rating": rating
        }  
        ```
    - è®€å–èˆŠæœ‰æª”æ¡ˆå…§å®¹ä¸¦åˆªé™¤èˆŠæª”æ¡ˆ
    - å°‡èˆŠè³‡æ–™èˆ‡æ–°å¢è³‡æ–™åˆä½µï¼Œä¸¦å»ºç«‹æ–°æª”æ¡ˆ 

